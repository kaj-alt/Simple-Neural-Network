{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "'Towards Data Science' transformer with floats.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bzXOUzlNqROr"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import math\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build the Transformer"
      ],
      "metadata": {
        "id": "MIcqGjH303jf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    \"\"\"\n",
        "    Model from \"A detailed guide to Pytorch's nn.Transformer() module.\", by\n",
        "    Daniel Melchor: https://medium.com/@danielmelchor/a-detailed-guide-to-pytorchs-nn-transformer-module-c80afbc9ffb1\n",
        "    \"\"\"\n",
        "    # Constructor\n",
        "    def __init__(\n",
        "        self,\n",
        "        embedding_nodes,\n",
        "        num_tokens,\n",
        "        dim_model,\n",
        "        num_heads,\n",
        "        num_encoder_layers,\n",
        "        num_decoder_layers,\n",
        "        dropout_p,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        # INFO\n",
        "        self.model_type = \"Transformer\"\n",
        "        self.dim_model = dim_model\n",
        "\n",
        "        # LAYERS\n",
        "        self.positional_encoder = PositionalEncoding(\n",
        "            dim_model=dim_model, dropout_p=dropout_p, max_len=5000\n",
        "        )\n",
        "        #self.embedding = nn.Embedding(num_tokens, dim_model)\n",
        "        self.embedding = nn.Linear(1,dim_model)\n",
        "\n",
        "        self.transformer = nn.Transformer(\n",
        "            d_model=dim_model,\n",
        "            nhead=num_heads,\n",
        "            num_encoder_layers=num_encoder_layers,\n",
        "            num_decoder_layers=num_decoder_layers,\n",
        "            dropout=dropout_p,\n",
        "        )\n",
        "        self.out = nn.Linear(dim_model, 1)\n",
        "        \n",
        "    def forward(self, src, tgt, tgt_mask=None, src_pad_mask=None, tgt_pad_mask=None):\n",
        "        # Src size must be (batch_size, src sequence length)\n",
        "        # Tgt size must be (batch_size, tgt sequence length)\n",
        "\n",
        "        # Embedding + positional encoding - Out size = (batch_size, sequence length, dim_model)\n",
        "        src = self.embedding(src) * math.sqrt(self.dim_model)\n",
        "        tgt = self.embedding(tgt) * math.sqrt(self.dim_model)\n",
        "        src = self.positional_encoder(src)\n",
        "        tgt = self.positional_encoder(tgt)\n",
        "        \n",
        "        # We could use the parameter batch_first=True, but our KDL version doesn't support it yet, so we permute\n",
        "        # to obtain size (sequence length, batch_size, dim_model),\n",
        "        src = src.permute(1,0,2)\n",
        "        tgt = tgt.permute(1,0,2)\n",
        "\n",
        "        # Transformer blocks - Out size = (sequence length, batch_size, num_tokens)\n",
        "        transformer_out = self.transformer(src, tgt, tgt_mask=tgt_mask, src_key_padding_mask=src_pad_mask, tgt_key_padding_mask=tgt_pad_mask)\n",
        "        out = self.out(transformer_out)\n",
        "        \n",
        "        return out\n",
        "      \n",
        "    def get_tgt_mask(self, size) -> torch.tensor:\n",
        "        # Generates a squeare matrix where the each row allows one word more to be seen\n",
        "        mask = torch.tril(torch.ones(size, size) == 1) # Lower triangular matrix\n",
        "        mask = mask.float()\n",
        "        mask = mask.masked_fill(mask == 0, float('-inf')) # Convert zeros to -inf\n",
        "        mask = mask.masked_fill(mask == 1, float(0.0)) # Convert ones to 0\n",
        "        \n",
        "        # EX for size=5:\n",
        "        # [[0., -inf, -inf, -inf, -inf],\n",
        "        #  [0.,   0., -inf, -inf, -inf],\n",
        "        #  [0.,   0.,   0., -inf, -inf],\n",
        "        #  [0.,   0.,   0.,   0., -inf],\n",
        "        #  [0.,   0.,   0.,   0.,   0.]]\n",
        "        \n",
        "        return mask\n",
        "    \n",
        "    def create_pad_mask(self, matrix: torch.tensor, pad_token: int) -> torch.tensor:\n",
        "        # If matrix = [1,2,3,0,0,0] where pad_token=0, the result mask is\n",
        "        # [False, False, False, True, True, True]\n",
        "        return (matrix == pad_token)"
      ],
      "metadata": {
        "id": "jnj5-5CNql4D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, dim_model, dropout_p, max_len):\n",
        "        super().__init__()\n",
        "        # Modified version from: https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
        "        # max_len determines how far the position can have an effect on a token (window)\n",
        "        \n",
        "        # Info\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "        \n",
        "        # Encoding - From formula\n",
        "        pos_encoding = torch.zeros(max_len, dim_model)\n",
        "        positions_list = torch.arange(0, max_len, dtype=torch.float).view(-1, 1) # 0, 1, 2, 3, 4, 5\n",
        "        division_term = torch.exp(torch.arange(0, dim_model, 2).float() * (-math.log(10000.0)) / dim_model) # 1000^(2i/dim_model)\n",
        "        \n",
        "        # PE(pos, 2i) = sin(pos/1000^(2i/dim_model))\n",
        "        pos_encoding[:, 0::2] = torch.sin(positions_list * division_term)\n",
        "        \n",
        "        # PE(pos, 2i + 1) = cos(pos/1000^(2i/dim_model))\n",
        "        pos_encoding[:, 1::2] = torch.cos(positions_list * division_term)\n",
        "        \n",
        "        # Saving buffer (same as parameter without gradients needed)\n",
        "        pos_encoding = pos_encoding.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer(\"pos_encoding\",pos_encoding)\n",
        "        \n",
        "    def forward(self, token_embedding: torch.tensor) -> torch.tensor:\n",
        "        # Residual connection + pos encoding\n",
        "        return self.dropout(token_embedding + self.pos_encoding[:token_embedding.size(0), :])"
      ],
      "metadata": {
        "id": "iBm2vm1GwshR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Data Generating & Batching Functions"
      ],
      "metadata": {
        "id": "XSOtopMp1D5F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_random_data(n):\n",
        "    SOS_token = np.array([2])\n",
        "    EOS_token = np.array([3])\n",
        "    length = 8\n",
        "\n",
        "    data = []\n",
        "\n",
        "    # 1,1,1,1,1,1 -> 1,1,1,1,1\n",
        "    for i in range(n // 3):\n",
        "        X = np.ones(length)\n",
        "        y = np.ones(length)\n",
        "        data.append([X, y])\n",
        "    \n",
        "    # 0,0,0,0 -> 0,0,0,0\n",
        "    for i in range(n // 3):\n",
        "        X = np.zeros(length)\n",
        "        y = np.zeros(length)\n",
        "        data.append([X, y])\n",
        "    \n",
        "    # 1,0,1,0 -> 1,0,1,0,1\n",
        "    for i in range(n // 3):\n",
        "        X = np.zeros(length)\n",
        "        start = np.random.randint(0,1)\n",
        "\n",
        "        X[start::2] = 1\n",
        "\n",
        "        y = np.zeros(length)\n",
        "        if X[-1] == 0:\n",
        "            y[::2] = 1\n",
        "        else:\n",
        "            y[1::2] = 1\n",
        "\n",
        "        data.append([X, y])\n",
        "    \n",
        "    np.random.shuffle(data)\n",
        "\n",
        "    return data"
      ],
      "metadata": {
        "id": "VUrocHEXzAba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def batchify_data(data, batch_size=16, padding=False, padding_token=-1):\n",
        "    batches = []\n",
        "    for idx in range(0, len(data), batch_size):\n",
        "        # We make sure we dont get the last bit if its not batch_size size\n",
        "        if idx + batch_size < len(data):\n",
        "            # Here you would need to get the max length of the batch,\n",
        "            # and normalize the length with the PAD token.\n",
        "            if padding:\n",
        "                max_batch_length = 0\n",
        "\n",
        "                # Get longest sentence in batch\n",
        "                for seq in data[idx : idx + batch_size]:\n",
        "                    if len(seq) > max_batch_length:\n",
        "                        max_batch_length = len(seq)\n",
        "\n",
        "                # Append X padding tokens until it reaches the max length\n",
        "                for seq_idx in range(batch_size):\n",
        "                    remaining_length = max_bath_length - len(data[idx + seq_idx])\n",
        "                    data[idx + seq_idx] += [padding_token] * remaining_length\n",
        "\n",
        "            batches.append(np.array(data[idx : idx + batch_size]).astype(np.float32))\n",
        "\n",
        "    print(f\"{len(batches)} batches of size {batch_size}\")\n",
        "\n",
        "    return batches"
      ],
      "metadata": {
        "id": "3GRZ-kXlzFVh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Training Functions"
      ],
      "metadata": {
        "id": "aVeBzFsH1noG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_loop(model, opt, loss_fn, dataloader):\n",
        "    \"\"\"\n",
        "    Method from \"A detailed guide to Pytorch's nn.Transformer() module.\", by\n",
        "    Daniel Melchor: https://medium.com/@danielmelchor/a-detailed-guide-to-pytorchs-nn-transformer-module-c80afbc9ffb1\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in dataloader:\n",
        "        X, y = batch[:, 0], batch[:, 1]\n",
        "        X, y = torch.tensor(X[:,:,None], dtype = torch.float).to(device), torch.tensor(y[:,:,None], dtype = torch.float).to(device)\n",
        "        # Now we shift the tgt by one so with the <SOS> we predict the token at pos 1\n",
        "        y_input = y[:,:-1]\n",
        "        y_expected = y[:,1:]\n",
        "        \n",
        "        # Get mask to mask out the next words\n",
        "        sequence_length = y_input.size(1)\n",
        "        tgt_mask = model.get_tgt_mask(sequence_length).to(device)\n",
        "\n",
        "        # Standard training except we pass in y_input and tgt_mask\n",
        "        pred = model(X, y_input, tgt_mask)\n",
        "\n",
        "        # Permute pred to have batch size first again\n",
        "        pred = pred.permute(1, 0, 2)      \n",
        "        loss = loss_fn(pred, y_expected)\n",
        "\n",
        "        opt.zero_grad()\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "    \n",
        "        total_loss += loss.detach().item()\n",
        "        \n",
        "    return total_loss / len(dataloader)"
      ],
      "metadata": {
        "id": "9saTNdZ80CZq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validation_loop(model, loss_fn, dataloader):\n",
        "    \"\"\"\n",
        "    Method from \"A detailed guide to Pytorch's nn.Transformer() module.\", by\n",
        "    Daniel Melchor: https://medium.com/@danielmelchor/a-detailed-guide-to-pytorchs-nn-transformer-module-c80afbc9ffb1\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            X, y = batch[:, 0], batch[:, 1]\n",
        "            X, y = torch.tensor(X[:,:,None], dtype=torch.float, device=device), torch.tensor(y[:,:,None], dtype=torch.float, device=device)\n",
        "\n",
        "            # Now we shift the tgt by one so with the <SOS> we predict the token at pos 1\n",
        "            y_input = y[:,:-1]\n",
        "            y_expected = y[:,1:]\n",
        "            \n",
        "            # Get mask to mask out the next words\n",
        "            sequence_length = y_input.size(1)\n",
        "            tgt_mask = model.get_tgt_mask(sequence_length).to(device)\n",
        "\n",
        "            # Standard training except we pass in y_input and src_mask\n",
        "            pred = model(X, y_input, tgt_mask)\n",
        "\n",
        "            # Permute pred to have batch size first again\n",
        "            pred = pred.permute(1, 0, 2) \n",
        "            loss = loss_fn(pred, y_expected)\n",
        "            total_loss += loss.detach().item()\n",
        "        \n",
        "    return total_loss / len(dataloader)"
      ],
      "metadata": {
        "id": "vBA_hZiG0W1H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fit(model, opt, loss_fn, train_dataloader, val_dataloader, epochs):\n",
        "    \"\"\"\n",
        "    Method from \"A detailed guide to Pytorch's nn.Transformer() module.\", by\n",
        "    Daniel Melchor: https://medium.com/@danielmelchor/a-detailed-guide-to-pytorchs-nn-transformer-module-c80afbc9ffb1\n",
        "    \"\"\"\n",
        "    \n",
        "    # Used for plotting later on\n",
        "    train_loss_list, validation_loss_list = [], []\n",
        "    \n",
        "    print(\"Training and validating model\")\n",
        "    for epoch in range(epochs):\n",
        "        print(\"-\"*25, f\"Epoch {epoch + 1}\",\"-\"*25)\n",
        "        \n",
        "        train_loss = train_loop(model, opt, loss_fn, train_dataloader)\n",
        "        train_loss_list += [train_loss]\n",
        "        \n",
        "        validation_loss = validation_loop(model, loss_fn, val_dataloader)\n",
        "        validation_loss_list += [validation_loss]\n",
        "        \n",
        "        print(f\"Training loss: {train_loss:.4f}\")\n",
        "        print(f\"Validation loss: {validation_loss:.4f}\")\n",
        "        print()\n",
        "        \n",
        "    return train_loss_list, validation_loss_list"
      ],
      "metadata": {
        "id": "1cou_oGA0b1E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Testing Functions"
      ],
      "metadata": {
        "id": "zovPVBI51-sj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(model, input_sequence, max_length=8, SOS_token=2, EOS_token=3):\n",
        "    \"\"\"\n",
        "    Method from \"A detailed guide to Pytorch's nn.Transformer() module.\", by\n",
        "    Daniel Melchor: https://medium.com/@danielmelchor/a-detailed-guide-to-pytorchs-nn-transformer-module-c80afbc9ffb1\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    \n",
        "    y_input = torch.tensor([[[SOS_token]]], dtype=torch.float, device=device)\n",
        "\n",
        "    num_tokens = len(input_sequence[0])\n",
        "\n",
        "    for _ in range(max_length):\n",
        "        # Get source mask\n",
        "        tgt_mask = model.get_tgt_mask(y_input.size(1)).to(device)\n",
        "        \n",
        "        pred = model(input_sequence, y_input, tgt_mask)\n",
        "        pred = pred.permute(1, 0, 2)\n",
        "        next_item = pred.view(-1)[-1] # num with highest probability\n",
        "        next_item = torch.tensor([[[next_item]]], device=device)\n",
        "\n",
        "        # Concatenate previous input with predicted best word\n",
        "        y_input = torch.cat((y_input, next_item), dim=1)\n",
        "\n",
        "    return y_input.view(-1).tolist()"
      ],
      "metadata": {
        "id": "rvHsOZu22BSF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Train the Model"
      ],
      "metadata": {
        "id": "sYnxi3tH0t70"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = generate_random_data(9000)\n",
        "val_data = generate_random_data(3000)\n",
        "\n",
        "train_dataloader = batchify_data(train_data)\n",
        "val_dataloader = batchify_data(val_data)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = Transformer(\n",
        "    embedding_nodes = 1000, num_tokens=4, dim_model=8, num_heads=2, num_encoder_layers=3, num_decoder_layers=3, dropout_p=0.1\n",
        ").to(device)\n",
        "opt = torch.optim.SGD(model.parameters(), lr=0.01)\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "train_loss_list, validation_loss_list = fit(model, opt, loss_fn, train_dataloader, val_dataloader, 50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wl4pfznF0lal",
        "outputId": "0c59bef6-b256-48d2-be58-f412fe33bd34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "562 batches of size 16\n",
            "187 batches of size 16\n",
            "Training and validating model\n",
            "------------------------- Epoch 1 -------------------------\n",
            "Training loss: 0.0538\n",
            "Validation loss: 0.0057\n",
            "\n",
            "------------------------- Epoch 2 -------------------------\n",
            "Training loss: 0.0144\n",
            "Validation loss: 0.0021\n",
            "\n",
            "------------------------- Epoch 3 -------------------------\n",
            "Training loss: 0.0076\n",
            "Validation loss: 0.0011\n",
            "\n",
            "------------------------- Epoch 4 -------------------------\n",
            "Training loss: 0.0051\n",
            "Validation loss: 0.0006\n",
            "\n",
            "------------------------- Epoch 5 -------------------------\n",
            "Training loss: 0.0039\n",
            "Validation loss: 0.0005\n",
            "\n",
            "------------------------- Epoch 6 -------------------------\n",
            "Training loss: 0.0030\n",
            "Validation loss: 0.0004\n",
            "\n",
            "------------------------- Epoch 7 -------------------------\n",
            "Training loss: 0.0023\n",
            "Validation loss: 0.0003\n",
            "\n",
            "------------------------- Epoch 8 -------------------------\n",
            "Training loss: 0.0021\n",
            "Validation loss: 0.0002\n",
            "\n",
            "------------------------- Epoch 9 -------------------------\n",
            "Training loss: 0.0017\n",
            "Validation loss: 0.0002\n",
            "\n",
            "------------------------- Epoch 10 -------------------------\n",
            "Training loss: 0.0015\n",
            "Validation loss: 0.0002\n",
            "\n",
            "------------------------- Epoch 11 -------------------------\n",
            "Training loss: 0.0013\n",
            "Validation loss: 0.0002\n",
            "\n",
            "------------------------- Epoch 12 -------------------------\n",
            "Training loss: 0.0012\n",
            "Validation loss: 0.0001\n",
            "\n",
            "------------------------- Epoch 13 -------------------------\n",
            "Training loss: 0.0011\n",
            "Validation loss: 0.0001\n",
            "\n",
            "------------------------- Epoch 14 -------------------------\n",
            "Training loss: 0.0011\n",
            "Validation loss: 0.0001\n",
            "\n",
            "------------------------- Epoch 15 -------------------------\n",
            "Training loss: 0.0010\n",
            "Validation loss: 0.0001\n",
            "\n",
            "------------------------- Epoch 16 -------------------------\n",
            "Training loss: 0.0008\n",
            "Validation loss: 0.0001\n",
            "\n",
            "------------------------- Epoch 17 -------------------------\n",
            "Training loss: 0.0008\n",
            "Validation loss: 0.0001\n",
            "\n",
            "------------------------- Epoch 18 -------------------------\n",
            "Training loss: 0.0006\n",
            "Validation loss: 0.0001\n",
            "\n",
            "------------------------- Epoch 19 -------------------------\n",
            "Training loss: 0.0007\n",
            "Validation loss: 0.0001\n",
            "\n",
            "------------------------- Epoch 20 -------------------------\n",
            "Training loss: 0.0006\n",
            "Validation loss: 0.0001\n",
            "\n",
            "------------------------- Epoch 21 -------------------------\n",
            "Training loss: 0.0006\n",
            "Validation loss: 0.0001\n",
            "\n",
            "------------------------- Epoch 22 -------------------------\n",
            "Training loss: 0.0005\n",
            "Validation loss: 0.0001\n",
            "\n",
            "------------------------- Epoch 23 -------------------------\n",
            "Training loss: 0.0005\n",
            "Validation loss: 0.0001\n",
            "\n",
            "------------------------- Epoch 24 -------------------------\n",
            "Training loss: 0.0004\n",
            "Validation loss: 0.0001\n",
            "\n",
            "------------------------- Epoch 25 -------------------------\n",
            "Training loss: 0.0005\n",
            "Validation loss: 0.0001\n",
            "\n",
            "------------------------- Epoch 26 -------------------------\n",
            "Training loss: 0.0004\n",
            "Validation loss: 0.0001\n",
            "\n",
            "------------------------- Epoch 27 -------------------------\n",
            "Training loss: 0.0004\n",
            "Validation loss: 0.0001\n",
            "\n",
            "------------------------- Epoch 28 -------------------------\n",
            "Training loss: 0.0004\n",
            "Validation loss: 0.0001\n",
            "\n",
            "------------------------- Epoch 29 -------------------------\n",
            "Training loss: 0.0003\n",
            "Validation loss: 0.0001\n",
            "\n",
            "------------------------- Epoch 30 -------------------------\n",
            "Training loss: 0.0004\n",
            "Validation loss: 0.0001\n",
            "\n",
            "------------------------- Epoch 31 -------------------------\n",
            "Training loss: 0.0005\n",
            "Validation loss: 0.0001\n",
            "\n",
            "------------------------- Epoch 32 -------------------------\n",
            "Training loss: 0.0003\n",
            "Validation loss: 0.0001\n",
            "\n",
            "------------------------- Epoch 33 -------------------------\n",
            "Training loss: 0.0003\n",
            "Validation loss: 0.0001\n",
            "\n",
            "------------------------- Epoch 34 -------------------------\n",
            "Training loss: 0.0003\n",
            "Validation loss: 0.0001\n",
            "\n",
            "------------------------- Epoch 35 -------------------------\n",
            "Training loss: 0.0003\n",
            "Validation loss: 0.0001\n",
            "\n",
            "------------------------- Epoch 36 -------------------------\n",
            "Training loss: 0.0003\n",
            "Validation loss: 0.0001\n",
            "\n",
            "------------------------- Epoch 37 -------------------------\n",
            "Training loss: 0.0003\n",
            "Validation loss: 0.0001\n",
            "\n",
            "------------------------- Epoch 38 -------------------------\n",
            "Training loss: 0.0003\n",
            "Validation loss: 0.0001\n",
            "\n",
            "------------------------- Epoch 39 -------------------------\n",
            "Training loss: 0.0002\n",
            "Validation loss: 0.0001\n",
            "\n",
            "------------------------- Epoch 40 -------------------------\n",
            "Training loss: 0.0002\n",
            "Validation loss: 0.0001\n",
            "\n",
            "------------------------- Epoch 41 -------------------------\n",
            "Training loss: 0.0002\n",
            "Validation loss: 0.0001\n",
            "\n",
            "------------------------- Epoch 42 -------------------------\n",
            "Training loss: 0.0003\n",
            "Validation loss: 0.0001\n",
            "\n",
            "------------------------- Epoch 43 -------------------------\n",
            "Training loss: 0.0002\n",
            "Validation loss: 0.0000\n",
            "\n",
            "------------------------- Epoch 44 -------------------------\n",
            "Training loss: 0.0003\n",
            "Validation loss: 0.0001\n",
            "\n",
            "------------------------- Epoch 45 -------------------------\n",
            "Training loss: 0.0002\n",
            "Validation loss: 0.0000\n",
            "\n",
            "------------------------- Epoch 46 -------------------------\n",
            "Training loss: 0.0002\n",
            "Validation loss: 0.0000\n",
            "\n",
            "------------------------- Epoch 47 -------------------------\n",
            "Training loss: 0.0001\n",
            "Validation loss: 0.0000\n",
            "\n",
            "------------------------- Epoch 48 -------------------------\n",
            "Training loss: 0.0002\n",
            "Validation loss: 0.0000\n",
            "\n",
            "------------------------- Epoch 49 -------------------------\n",
            "Training loss: 0.0002\n",
            "Validation loss: 0.0001\n",
            "\n",
            "------------------------- Epoch 50 -------------------------\n",
            "Training loss: 0.0002\n",
            "Validation loss: 0.0000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test the Model"
      ],
      "metadata": {
        "id": "w7iSUQEE2Ozn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#examples = torch.empty((6, 10, 1), dtype=torch.float, device=device)\n",
        "examples = []\n",
        "examples.append(torch.tensor([[[0], [0], [0], [0], [0], [0], [0], [0]]], dtype=torch.float, device=device))\n",
        "examples.append(torch.tensor([[[1], [1], [1], [1], [1], [1], [1], [1]]], dtype=torch.float, device=device))\n",
        "examples.append(torch.tensor([[[1], [0], [1], [0], [1], [0], [1], [0]]], dtype=torch.float, device=device))\n",
        "examples.append(torch.tensor([[[0], [1], [0], [1], [0], [1], [0], [1]]], dtype=torch.float, device=device))\n",
        "examples.append(torch.tensor([[[0], [1], [0], [1], [0], [1], [0], [1], [0], [1], [0]]], dtype=torch.float, device=device))\n",
        "examples.append(torch.tensor([[[0], [1]]], dtype=torch.float, device=device))\n",
        "\n",
        "for idx, example in enumerate(examples):\n",
        "    result = predict(model, example)\n",
        "    print(f\"Example {idx}\")\n",
        "    print(f\"Input: {example.view(-1).tolist()}\")\n",
        "    print(f\"Continuation: {result[1:]}\")\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0PexNxPE0rFc",
        "outputId": "4abfbf89-09c3-4479-e441-ce55fc363872"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example 0\n",
            "Input: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Continuation: [-0.013088524341583252, -0.020528465509414673, -0.021296530961990356, -0.02069440484046936, -0.01956656575202942, -0.01834353804588318, -0.017214089632034302, -0.016219735145568848]\n",
            "\n",
            "Example 1\n",
            "Input: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "Continuation: [1.006716251373291, 1.0066148042678833, 1.006735920906067, 1.0067850351333618, 1.0068104267120361, 1.0068258047103882, 1.0068358182907104, 1.006842851638794]\n",
            "\n",
            "Example 2\n",
            "Input: [1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0]\n",
            "Continuation: [-0.005157768726348877, 0.9995417594909668, -0.006770044565200806, 1.0031254291534424, -0.005970358848571777, 1.0035932064056396, -0.005493223667144775, 1.0035983324050903]\n",
            "\n",
            "Example 3\n",
            "Input: [0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0]\n",
            "Continuation: [-0.005157798528671265, 0.9995418787002563, -0.006770014762878418, 1.0031254291534424, -0.00597032904624939, 1.0035932064056396, -0.005493223667144775, 1.0035983324050903]\n",
            "\n",
            "Example 4\n",
            "Input: [0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0]\n",
            "Continuation: [-0.006120949983596802, 0.9963647127151489, -0.007592380046844482, 1.0007648468017578, -0.006859898567199707, 1.0014851093292236, -0.006444483995437622, 1.001605749130249]\n",
            "\n",
            "Example 5\n",
            "Input: [0.0, 1.0]\n",
            "Continuation: [-0.005157768726348877, 0.9995418787002563, -0.006770044565200806, 1.0031254291534424, -0.005970358848571777, 1.0035932064056396, -0.005493223667144775, 1.0035983324050903]\n",
            "\n"
          ]
        }
      ]
    }
  ]
}